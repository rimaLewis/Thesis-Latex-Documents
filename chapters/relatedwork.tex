\ifgerman{\chapter{Verwandte Arbeiten}}{\chapter{Related Work}}
\label{relatedwork}

To the best of our knowledge, visualization specifically based on performance-influence models has not been an area of research up until now, although we have studies related to performance visualization techniques in recent past that aids developers and analysts in detecting flaws in the performance of the software by visualizing performance graphically. Visualization of performance also aids in improving the time and energy efficiency of the software.


The paper by Isaacs \textit{et al}.~\cite{DBLP:conf/vissym/IsaacsGJGB0HB14}  mainly focuses on the current state of the art of performance visualization. This paper surveys existing work on performance visualization, and categorize the goals that these performance visualization techniques can answer. The purpose of this survey is to introduce state of the art for information visualization, which aids domain experts in exploring tools and methods to analyze their data.
The results from the survey are organized into areas depending on which the visualization is constructed and describe the state of art research for each area. However, the number of domain experts available is small, which made it difficult to conduct a usability study. Therefore, an alternative approach required a small number of visualization and domain experts, which were more feasible for a usability study.


Another research by Haynes \textit{et al}.~\cite{DBLP:conf/cluster/HaynesCR01} presents a 3-D visualization tool to visually represent the performance data from a large scale cluster for analyzing. Differing from the study by Isaacs et al., Haynes focused on a particular visualization technique and the visualization displays data in the context of complex cluster interconnection topologies. It aids analysts to discover the cause of issues ranging from communication bottlenecks to hardware errors. While Isaacs et al., did not conduct a usability study, Haynes evaluated the effectiveness of the visualization tool on clusters in two separate instances. The first instance of usability shows that using visualization can minimize the time taken to diagnose hardware problems in a large system. The second instance of usability demonstrates that visualization can provide insight for understanding systems and job performance. 

A similar study by M{\"{u}}ller \textit{et al}.~\cite{DBLP:conf/parco/MullerKJLBMN07} presents scalability studies on performance analysis tool Vampir and VampirTrace. Unlike in the study by Haynes \textit{et al.},  M{\"{u}}ller focuses on scalability studies of dedicated tools in the Vampir tools family. The usability study and analysis of these tools are done on real applications taken from the SpecMPI benchmark suite. Vampir is a well known performance analysis framework. Vampir and VampirServer provide an interactive visualization of dynamic program behavior. They depend on loading the trace data to the main memory completely to begin the performance analysis session. The evaluation with VampirServer displayed good results, provided enough distributed memory. Hence, the software architecture is suitable for distributed memory platforms.

A similar work done by Bhatele \textit{et al}.~\cite{DBLP:conf/sc/BhateleGIGSBH12}, aims at visualizing performance data of large scale adaptive applications. In comparison to studies by Haynes  \textit{et al}. and by M{\"{u}}ller \textit{et al}. this study presents a scalable visualization technique that combines hardware and communication data providing an extensive diagnosis of detailed data collected from a dynamically structured AMR library. In addition to performance measurements, the visualization assisted in the diagnosis of a scalability problem that caused a bottleneck in the AMR library. The evaluation showed that the mitigation strategy improves the performance of the AMR library by 22\% for a 65,536 core run on a Blue Gene/P system. 

A study which used a questionnaire as a method of evaluation of research is by Herman \textit{et al.}~\cite{DBLP:article/tog/Herman16}, where they identify potential differences between the performance of user towards static perspective views and interactive 3-D visualizations. Experimental tools based on web frameworks were used for this test. To identify the differences an initial questionnaire, and several training and experimental tasks were conducted. The duration of tasks (time measurements), the correctness
of answers, errors and subjective evaluation of the difficulty of
individual tasks were evaluated and analyzed. The results from the questionnaire and other experiments suggested that in general, the participants working with static perspective views reached better results with fewer errors and were also faster compared to interactive 3-D visualizations.